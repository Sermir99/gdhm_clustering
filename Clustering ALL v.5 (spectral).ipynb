{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #\n",
    "import numpy as np #\n",
    "#from pandas_profiling import ProfileReport\n",
    "##\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster.hierarchy import * \n",
    "from scipy.spatial.distance import euclidean#\n",
    "from scipy.stats import norm#\n",
    "from scipy import stats #\n",
    "\n",
    "import skfuzzy as fuzz\n",
    "\n",
    "##\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cluster import KMeans \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "##\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from dtaidistance import dtw\n",
    "import fastdtw\n",
    "from fastdtw import fastdtw\n",
    "import math\n",
    "import hdbscan\n",
    "##\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import rc\n",
    "from matplotlib import patches\n",
    "import plotly as py\n",
    "import seaborn as sns\n",
    "\n",
    "import colorlover as cl\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import plotly.graph_objects as go\n",
    "##\n",
    "import re \n",
    "import os\n",
    "if not os.path.exists(\"images\"): os.mkdir(\"images\")\n",
    "##\n",
    "%matplotlib inline \n",
    "\n",
    "from s_dbw import SD\n",
    "\n",
    "import random\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df1 = pd.read_excel('input/37 variables LH300 для кластеризации.xlsx')\n",
    "target1 = df1.iloc[:,0]\n",
    "prepared1 = df1.iloc[:,2:]\n",
    "target1.shape, prepared1.shape\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('input/37 variables opt200 для кластеризации.xlsx')\n",
    "target = df.iloc[20:,1].reset_index(drop=True)\n",
    "prepared = df.iloc[20:,3:-5].reset_index(drop = True)\n",
    "\n",
    "print(target.shape, prepared.shape,df.loc[20][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prepared.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = prepared.iloc[:,7:8].reset_index(drop = True).copy()\n",
    "feature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#загружаем библиотеку препроцесинга данных эта библиотека автоматически приведет данные к нормальным значениям\n",
    "scaler = MinMaxScaler()\n",
    "dataNorm = scaler.fit_transform(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set_style(\"white\")\n",
    "N = 7\n",
    "\n",
    "x1 = pd.DataFrame(dataNorm)\n",
    "x2 = feature\n",
    "\n",
    "kwargs = dict(hist_kws={'alpha':.3}, kde_kws={'linewidth':2})\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7,5.5), dpi= 80)\n",
    "sns.distplot(x1, color=\"dodgerblue\", label=\"Исходное распределение\", **kwargs)\n",
    "sns.distplot(x2, color=\"orange\", label=\"Масштабированные данные\", **kwargs)\n",
    "plt.title(prepared.columns[N],fontsize =20)\n",
    "plt.ylabel('Density probability',fontdict = dict(fontsize = 20))\n",
    "plt.xlabel('Corey Water',fontdict = dict(fontsize = 20))\n",
    "plt.legend(fontsize = 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELBOW METHOD OF DETERMINATION NUMBER OF CLUSTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.cm as cm\n",
    "from s_dbw import S_Dbw\n",
    "\n",
    "def elbow(df,highest):\n",
    "    '''\n",
    "    Функция строит метрики оценки кластеров\n",
    "    \n",
    "    df - pd.DataFrame парметров ГДМ\n",
    "    highest- верхняя граница колиества кластеров\n",
    "    \n",
    "    '''\n",
    "    range_n_clusters = np.arange(2,highest,1)\n",
    "    \n",
    "    coeff, Sum_of_squared_distances, SDbw = [],[],[]\n",
    "    \n",
    "    scaler =  MinMaxScaler()\n",
    "    df = scaler.fit_transform(df)\n",
    "    \n",
    "    for n_clusters in range_n_clusters:\n",
    "        \n",
    "        clusterer = KMeans(n_clusters=n_clusters,tol=0.01, n_init=500)#,random_state=42)\n",
    "        cluster_labels = clusterer.fit_predict(df)       \n",
    "        \n",
    "        silhouette_avg = silhouette_score(df, cluster_labels)  \n",
    "        score = S_Dbw(df, \n",
    "                      cluster_labels,\n",
    "                      centers_id=None,\n",
    "                      method='Halkidi',\n",
    "                      alg_noise='bind',\n",
    "                      centr='mean', \n",
    "                      nearest_centr=True,                       \n",
    "                      metric='euclidean')\n",
    "        #print(score)\n",
    "        coeff.append(silhouette_avg)\n",
    "        Sum_of_squared_distances.append(clusterer.inertia_)\n",
    "        SDbw.append(score)  \n",
    "        \n",
    "    \n",
    "    y1 = coeff\n",
    "    y2 = Sum_of_squared_distances\n",
    "    y3 = SDbw\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    modes = 'markers+lines'\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x = range_n_clusters,\n",
    "            y = y1,\n",
    "            #showlegend = False,\n",
    "            name = 'Silhouette Score',\n",
    "            mode = modes,\n",
    "            marker = dict(color = 'red'),\n",
    "             yaxis='y1'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x = range_n_clusters,\n",
    "            y = y2,\n",
    "            #showlegend = False,\n",
    "            name = 'Sum of squared distances',\n",
    "            mode = modes,\n",
    "            marker = dict(color = 'green'),\n",
    "            yaxis='y2'\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x = range_n_clusters,\n",
    "            y = y3,\n",
    "            #showlegend = False,\n",
    "            name = 'SD validity index',\n",
    "            mode = modes,\n",
    "            marker = dict(color = 'blue'),\n",
    "            yaxis='y3'\n",
    "        )\n",
    "    ) \n",
    "    \n",
    "    fig.update_layout(\n",
    "        yaxis = dict(\n",
    "            #title=\"yaxis title\",\n",
    "            titlefont=dict(color=\"red\"),\n",
    "            tickfont=dict(color=\"red\")\n",
    "        ),\n",
    "        yaxis2 = dict(\n",
    "            #title=\"yaxis2 title\",\n",
    "            titlefont=dict(color=\"green\"),\n",
    "            tickfont=dict(color=\"green\"),\n",
    "            anchor=\"free\",\n",
    "            overlaying=\"y\",\n",
    "            side=\"right\",\n",
    "            position=1\n",
    "        ),        \n",
    "        yaxis3 = dict(\n",
    "            #title=\"yaxis3 title\",\n",
    "            titlefont=dict(color=\"blue\"),\n",
    "            tickfont=dict(color=\"blue\"),\n",
    "            anchor=\"free\",\n",
    "            overlaying=\"y\",\n",
    "            side=\"right\",\n",
    "            position=0\n",
    "        ),\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"right\",\n",
    "            x=1\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return fig.show() \n",
    "   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "color = list(sns.color_palette(n_colors = N).as_hex())\n",
    "color_palette = pd.DataFrame([color]).T\n",
    "color_palette.index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cc(df,ind_centroids,flag):\n",
    "    '''\n",
    "    df - датафрейм после кластеризации \n",
    "    ind_cetroids - список индексов центроидных моделей\n",
    "    '''\n",
    "    cl = ['центроид кластера '+ str(x+1) for x in range(len(ind_centroids))]   \n",
    "    if flag == True:\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        dataNorm = scaler.fit_transform(df)   \n",
    "        fig = go.Figure()\n",
    "        for j, jtem in enumerate(df.columns[:-2]):\n",
    "            input_ = []\n",
    "\n",
    "            for x in ind_centroids:\n",
    "                input_.append(dataNorm[x][j]) \n",
    "\n",
    "            fig.add_trace(go.Bar(\n",
    "                x=cl,\n",
    "                y=input_,\n",
    "                name = jtem\n",
    "            ))\n",
    "            fig.update_layout(barmode='group')\n",
    "            \n",
    "        fig.update_layout(title = 'Центроидные модели. Нормированные значения ручек')\n",
    "        \n",
    "    else:\n",
    "        fig = go.Figure()\n",
    "        for j, jtem in enumerate(df.columns[:-2]):\n",
    "            input_ = []\n",
    "\n",
    "            for x in ind_centroids:\n",
    "                input_.append(df.loc[x][jtem]) \n",
    "\n",
    "            fig.add_trace(go.Bar(\n",
    "                x=cl,\n",
    "                y=input_,\n",
    "                name = jtem\n",
    "            ))\n",
    "            fig.update_layout(barmode='group')\n",
    "            \n",
    "        fig.update_layout(title = 'Центроидные модели. Исходные значения ручек')\n",
    "    return fig.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg(df,id_, metric = 'euclidean'):\n",
    "    '''\n",
    "    df - pd.DataFrame features\n",
    "    n_clusters - указать количество кластеров\n",
    "    flag - если True, то рисует картинки. если False - то не рисует\n",
    "    \n",
    "    '''\n",
    "    scaler =  RobustScaler()# MinMaxScaler()\n",
    "    dataNorm = scaler.fit_transform(df)\n",
    "    \n",
    "    elbow(df,2,10)\n",
    "    n_clusters = int(input())\n",
    "    \n",
    "    agg = AgglomerativeClustering(n_clusters = n_clusters, affinity=metric, linkage='complete').fit(dataNorm)\n",
    "    models = pd.DataFrame(id_,columns=['UWI'])\n",
    "    output = pd.concat([df,pd.DataFrame(agg.labels_ +1,columns=['label']),models],axis=1)\n",
    "    \n",
    "    fig1 = px.pie(output,\n",
    "                  names='label',\n",
    "                  color = 'label',\n",
    "                  color_discrete_map = color_palette.loc[:n_clusters].to_dict()[0])\n",
    "                 \n",
    "    fig1.update_layout(title='% из общей ('+ str(len(df)) +') выборки в каждом кластере')\n",
    "    fig1.show()\n",
    "    \n",
    "    divided = []     \n",
    "    for j,jtem in enumerate(np.unique(agg.labels_ +1)):\n",
    "        divided.append(output[output.label == jtem])  \n",
    "        \n",
    "    for i in range(output.shape[1]-1):\n",
    "        \n",
    "        feature = output.columns[i]\n",
    "        fig = go.Figure()\n",
    "        #fig = make_subplots(rows=1, cols=2)\n",
    "        \n",
    "        for k in range(len(divided)):  \n",
    "            #print(divided[k].columns[i])\n",
    "            ecdf = ECDF(divided[k].iloc[:,i].values)           \n",
    "           \n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=ecdf.x,\n",
    "                           y=ecdf.y,\n",
    "                           mode='markers',\n",
    "                           name = str(k+1),\n",
    "                           marker = dict(size = 4,color = color_palette.loc[:n_clusters].to_dict()[0][k+1]),\n",
    "                          ),\n",
    "                           \n",
    "            ) \n",
    "\n",
    "            \n",
    "            fig.update_xaxes(title = 'Значение параметра '+ str(divided[k].columns[i]))\n",
    "            fig.update_yaxes(title = 'Вероятность, доля ')\n",
    "            fig.update_layout(title = 'Empirical Cumulative Distribution Function of ' + str(divided[k].columns[i]))\n",
    "            #fig.write_image('images/'+'Empirical Cumulative Distribution Function of ' + str(divided[k].columns[i])+'.jpeg')\n",
    "                      \n",
    "        fig.show()\n",
    "            \n",
    "    closest = [] \n",
    "    for cl in range(n_clusters):\n",
    "        \n",
    "        df_where_only_i_cluster = output[output.label == cl+1].drop(columns=output.columns[-1])\n",
    "        centroid = df_where_only_i_cluster.mean()\n",
    "        \n",
    "        centroid = np.array(centroid).reshape(1, -1)\n",
    "        centroid, _ = pairwise_distances_argmin_min(centroid,df_where_only_i_cluster)\n",
    "        closest.append(df_where_only_i_cluster.iloc[centroid].index[0])\n",
    "        \n",
    "    print ('Имена центроидных моделей')\n",
    "    for x in closest:        \n",
    "        print(id_[x])\n",
    "        \n",
    "    return output, closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "def km(df,id_):\n",
    "    '''\n",
    "    df - нормализованный np.array \n",
    "    names - список идентификаторов для датасета \n",
    "    n_clusters - указать количество кластеров\n",
    "    flag - если True, то рисует картинки. если False - то не рисует\n",
    "    \n",
    "    '''    \n",
    "    #загружаем библиотеку препроцесинга данных эта библиотека автоматически приведет данные к нормальным значениям\n",
    "    \n",
    "    scaler =  MinMaxScaler()\n",
    "    \n",
    "    dataNorm = scaler.fit_transform(df)\n",
    "    \n",
    "    elbow(df,10)\n",
    "    n_clusters = int(input())\n",
    "    km = KMeans(n_clusters = n_clusters,tol=0.01, n_init=500, random_state=42).fit(dataNorm)   \n",
    "    \n",
    "    closest, _ = pairwise_distances_argmin_min(km.cluster_centers_, dataNorm)\n",
    "    print(closest)\n",
    "    print ('Имена центроидных моделей')\n",
    "    for x in closest:        \n",
    "        print(id_[x])\n",
    "        \n",
    "    models = pd.DataFrame(id_,columns=['UWI'])\n",
    "    output = pd.concat([df,pd.DataFrame(km.labels_ +1,columns=['label']),models],axis=1)\n",
    "    \n",
    "    cl = ['Кластер #'+ str(x+1) for x in range(len(closest))]\n",
    "   # print(Counter(model))\n",
    "    \n",
    "    \n",
    "    fig1 = px.pie(output,\n",
    "                  names='label',\n",
    "                  color = 'label',\n",
    "                  color_discrete_map = color_palette.loc[:n_clusters].to_dict()[0])\n",
    "                 \n",
    "    fig1.update_layout(title='% из общей ('+ str(len(df)) +') выборки в каждом кластере')\n",
    "    fig1.show()\n",
    "    \n",
    "    divided = []     \n",
    "    for j,jtem in enumerate(np.unique(km.labels_ +1)):\n",
    "        divided.append(output[output.label == jtem])  \n",
    "    \n",
    "    for i in range(output.shape[1]-1):        \n",
    "        feature = output.columns[i]  \n",
    "        \n",
    "        fig = make_subplots(rows=1, cols=2) \n",
    "        \n",
    "        for k in range(len(divided)):  \n",
    "            #print(divided[k].columns[i])\n",
    "            ecdf = ECDF(divided[k].iloc[:,i].values)           \n",
    "           \n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=ecdf.x,\n",
    "                    y=ecdf.y,\n",
    "                    mode='markers',\n",
    "                    legendgroup=\"group\",\n",
    "                    #name = str(k+1),\n",
    "                    marker = dict(size = 4,color = color_palette.loc[:n_clusters].to_dict()[0][k+1] ),\n",
    "                ),\n",
    "                row = 1, col = 1\n",
    "            )\n",
    "            #x_ = \n",
    "            #y_ = \n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x = [divided[k].loc[closest[k]][feature]],\n",
    "                    y = [ecdf.y[list(ecdf.x).index(divided[k].loc[closest[k]][feature])]],\n",
    "                    mode='markers',\n",
    "                    showlegend = False,\n",
    "                    marker_symbol='x',\n",
    "                    marker = dict(size = 15,                                  \n",
    "                                  color = color_palette.loc[:n_clusters].to_dict()[0][k+1] ),\n",
    "                    ),\n",
    "                    row = 1, col = 1\n",
    "                )             \n",
    "              \n",
    "            fig.add_trace(\n",
    "                go.Box(\n",
    "                    name = cl[k],\n",
    "                    y = ecdf.x,\n",
    "                    boxpoints='suspectedoutliers',\n",
    "                    legendgroup=\"group\",   \n",
    "                    showlegend=False,\n",
    "                    marker_color = color_palette.loc[:n_clusters].to_dict()[0][k+1],\n",
    "                ),\n",
    "                row = 1, col =2\n",
    "            )\n",
    "            #print(divided[k].loc[closest[k]][feature],ecdf.y[list(ecdf.x).index(divided[k].loc[closest[k]][feature])])\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x = [cl[k]],\n",
    "                    y = [divided[k].loc[closest[k]][feature]],\n",
    "                    mode='markers',\n",
    "                    \n",
    "                    showlegend = False,\n",
    "                    marker_symbol='x',\n",
    "                    marker = dict(size = 15,                                  \n",
    "                                  color = color_palette.loc[:n_clusters].to_dict()[0][k+1] ),\n",
    "                    ),\n",
    "                    row = 1, col = 2\n",
    "                )             \n",
    "\n",
    "            #fig.update_xaxes(title = 'Значение параметра '+ str(divided[k].columns[i]))\n",
    "            fig.update_layout(\n",
    "                xaxis1=dict(title = \"Значение параметра\"),\n",
    "                xaxis2=dict(title = \"Группа данных\"),\n",
    "                yaxis1=dict(title=\"Доля данных\"),\n",
    "                yaxis2=dict(title=\"Значение параметра\"))\n",
    "            fig.update_layout(title = 'Empirical Cumulative Distribution Function and Box-plots for each cluster ' + str(divided[k].columns[i]))\n",
    "            #fig.write_image('images/'+'Empirical Cumulative Distribution Function of ' + str(divided[k].columns[i])+'.jpeg')\n",
    "\n",
    "        fig.show()\n",
    "        \n",
    "    return output,closest,km.cluster_centers_,divided\n",
    "    #return divided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "def cm(df,id_, m=1.5):\n",
    "    '''\n",
    "    df - нормализованный np.array \n",
    "    names - список идентификаторов для датасета \n",
    "    n_clusters - указать количество кластеров\n",
    "    flag - если True, то рисует картинки. если False - то не рисует\n",
    "    \n",
    "    '''    \n",
    "    #загружаем библиотеку препроцесинга данных эта библиотека автоматически приведет данные к нормальным значениям\n",
    "    \n",
    "    scaler =  MinMaxScaler()\n",
    "    \n",
    "    dataNorm = scaler.fit_transform(df)\n",
    "    \n",
    "    elbow(df,10)\n",
    "    n_clusters = int(input())\n",
    "    cntr, u_orig, _, _, _, _, fpc = fuzz.cluster.cmeans(df.T, n_clusters, m=m, error=0.1, maxiter=10000, metric='cosine')\n",
    "    closest = []\n",
    "#     print(np.unique(u_orig.argmax(axis=0)))\n",
    "    try:\n",
    "        for j in range(n_clusters):\n",
    "            closest.append(pd.DataFrame(df)[u_orig.argmax(axis=0) == j].index[pairwise_distances_argmin_min([cntr[j]], df[u_orig.argmax(axis=0) == j])[0]])\n",
    "        closest = np.stack(closest).reshape(-1)\n",
    "    except:\n",
    "        return [u_orig]\n",
    "    print(closest)\n",
    "    print ('Имена центроидных моделей')\n",
    "    for x in closest:        \n",
    "        print(id_[x])\n",
    "        \n",
    "    models = pd.DataFrame(id_,columns=['UWI'])\n",
    "    output = pd.concat([df,pd.DataFrame(u_orig.argmax(axis=0)+1,columns=['label']),models],axis=1)\n",
    "    \n",
    "    cl = ['Кластер #'+ str(x+1) for x in range(len(closest))]\n",
    "   # print(Counter(model))\n",
    "    \n",
    "    \n",
    "    fig1 = px.pie(output,\n",
    "                  names='label',\n",
    "                  color = 'label',\n",
    "                  color_discrete_map = color_palette.loc[:n_clusters].to_dict()[0])\n",
    "                 \n",
    "    fig1.update_layout(title='% из общей ('+ str(len(df)) +') выборки в каждом кластере')\n",
    "    fig1.show()\n",
    "    \n",
    "    divided = []     \n",
    "    for j,jtem in enumerate(np.unique(u_orig.argmax(axis=0) +1)):\n",
    "        divided.append(output[output.label == jtem])  \n",
    "    \n",
    "    for i in range(output.shape[1]-1):        \n",
    "        feature = output.columns[i]  \n",
    "        \n",
    "        fig = make_subplots(rows=1, cols=2) \n",
    "        \n",
    "        for k in range(len(divided)):  \n",
    "            #print(divided[k].columns[i])\n",
    "            ecdf = ECDF(divided[k].iloc[:,i].values)           \n",
    "           \n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=ecdf.x,\n",
    "                    y=ecdf.y,\n",
    "                    mode='markers',\n",
    "                    legendgroup=\"group\",\n",
    "                    #name = str(k+1),\n",
    "                    marker = dict(size = 4,color = color_palette.loc[:n_clusters].to_dict()[0][k+1] ),\n",
    "                ),\n",
    "                row = 1, col = 1\n",
    "            )\n",
    "            #x_ = \n",
    "            #y_ = \n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x = [divided[k].loc[closest[k]][feature]],\n",
    "                    y = [ecdf.y[list(ecdf.x).index(divided[k].loc[closest[k]][feature])]],\n",
    "                    mode='markers',\n",
    "                    showlegend = False,\n",
    "                    marker_symbol='x',\n",
    "                    marker = dict(size = 15,                                  \n",
    "                                  color = color_palette.loc[:n_clusters].to_dict()[0][k+1] ),\n",
    "                    ),\n",
    "                    row = 1, col = 1\n",
    "                )             \n",
    "              \n",
    "            fig.add_trace(\n",
    "                go.Box(\n",
    "                    name = cl[k],\n",
    "                    y = ecdf.x,\n",
    "                    boxpoints='suspectedoutliers',\n",
    "                    legendgroup=\"group\",   \n",
    "                    showlegend=False,\n",
    "                    marker_color = color_palette.loc[:n_clusters].to_dict()[0][k+1],\n",
    "                ),\n",
    "                row = 1, col =2\n",
    "            )\n",
    "            #print(divided[k].loc[closest[k]][feature],ecdf.y[list(ecdf.x).index(divided[k].loc[closest[k]][feature])])\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x = [cl[k]],\n",
    "                    y = [divided[k].loc[closest[k]][feature]],\n",
    "                    mode='markers',\n",
    "                    \n",
    "                    showlegend = False,\n",
    "                    marker_symbol='x',\n",
    "                    marker = dict(size = 15,                                  \n",
    "                                  color = color_palette.loc[:n_clusters].to_dict()[0][k+1] ),\n",
    "                    ),\n",
    "                    row = 1, col = 2\n",
    "                )             \n",
    "\n",
    "            #fig.update_xaxes(title = 'Значение параметра '+ str(divided[k].columns[i]))\n",
    "            fig.update_layout(\n",
    "                xaxis1=dict(title = \"Значение параметра\"),\n",
    "                xaxis2=dict(title = \"Группа данных\"),\n",
    "                yaxis1=dict(title=\"Доля данных\"),\n",
    "                yaxis2=dict(title=\"Значение параметра\"))\n",
    "            fig.update_layout(title = 'Empirical Cumulative Distribution Function and Box-plots for each cluster ' + str(divided[k].columns[i]))\n",
    "            #fig.write_image('images/'+'Empirical Cumulative Distribution Function of ' + str(divided[k].columns[i])+'.jpeg')\n",
    "\n",
    "        fig.show()\n",
    "        \n",
    "    return output,closest,dataNorm[closest],divided\n",
    "    #return divided\n",
    "    \n",
    "    \n",
    "    \n",
    "def cm_select(df,id_):\n",
    "    '''\n",
    "    df - нормализованный np.array \n",
    "    names - список идентификаторов для датасета \n",
    "    n_clusters - указать количество кластеров\n",
    "    flag - если True, то рисует картинки. если False - то не рисует\n",
    "    \n",
    "    '''    \n",
    "    #загружаем библиотеку препроцесинга данных эта библиотека автоматически приведет данные к нормальным значениям\n",
    "    \n",
    "    scaler =  MinMaxScaler()\n",
    "    \n",
    "    dataNorm = scaler.fit_transform(df)\n",
    "    \n",
    "    elbow(df,10)\n",
    "    n_clusters = int(input())\n",
    "    cmeans_df = pd.DataFrame()\n",
    "    \n",
    "    tsne = TSNE(n_components = 2,\n",
    "                learning_rate=100,\n",
    "                early_exaggeration=12.0,\n",
    "                metric = 'cosine',\n",
    "                perplexity= 50,\n",
    "                n_iter = 1000,            \n",
    "                random_state =42)\n",
    "\n",
    "    tsne_model=tsne.fit_transform(dataNorm)\n",
    "    x_axis=tsne_model[:,0]\n",
    "    y_axis=tsne_model[:,1]\n",
    "    \n",
    "    for k,m in enumerate([1.001 + 0.1*i for i in  range(25)]):\n",
    "        cntr, u_orig, _, _, _, _, fpc = fuzz.cluster.cmeans(dataNorm.T, n_clusters, m=m, error=0.1, maxiter=10000, metric='cosine')\n",
    "        closest = []\n",
    "    #     print(np.unique(u_orig.argmax(axis=0)))\n",
    "        try:\n",
    "            for j in range(n_clusters):\n",
    "                closest.append(pd.DataFrame(dataNorm)[u_orig.argmax(axis=0) == j].index[pairwise_distances_argmin_min([cntr[j]], df[u_orig.argmax(axis=0) == j])[0]])\n",
    "            closest = np.stack(closest).reshape(-1)\n",
    "        except:\n",
    "            return [u_orig]\n",
    "\n",
    "        models = pd.DataFrame(id_,columns=['UWI'])\n",
    "        output = pd.concat([df,pd.DataFrame(u_orig.argmax(axis=0)+1,columns=['label']),models],axis=1)\n",
    "        \n",
    "        dist = [np.linalg.norm(dataNorm[closest[0]] - dataNorm[closest[1]]), np.linalg.norm(dataNorm[closest[0]] - dataNorm[closest[2]]), np.linalg.norm(dataNorm[closest[1]] - dataNorm[closest[2]])]\n",
    "        cmeans_df = cmeans_df.append([[m, closest, fpc, max(dist), np.min(dist)], ])\n",
    "        print()\n",
    "        if k%5==0:\n",
    "#             print(item + ' ' + str(perp) + ' ' + str(output_cmeans[2].mean()))\n",
    "            fig = go.Figure()\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x = x_axis,\n",
    "                    y = y_axis,\n",
    "                    mode = 'markers',\n",
    "                    marker= dict(color = output.label)\n",
    "                )\n",
    "            )\n",
    "            fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x = x_axis[closest],\n",
    "                y = y_axis[closest],\n",
    "                mode = 'markers',\n",
    "                marker = dict(symbol = 'x-open-dot', \n",
    "                    size = 30,\n",
    "        #             line_color='black',\n",
    "                    color='black',\n",
    "                    )\n",
    "            ))\n",
    "\n",
    "            fig.update_layout(title = 'cosine' + ' ' + str(50) + ' ' + str(fpc))\n",
    "            fig.show()\n",
    "    cmeans_df.columns = ['m', 'centroids', 'fpc', 'max euclidean dist', 'mean euclidean dist']\n",
    "    print(cmeans_df)\n",
    "    fig = px.line(cmeans_df, x='m', y='fpc', title='Четкость описания данных')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "def spec(df,id_):\n",
    "    \n",
    "    scaler =  MinMaxScaler()\n",
    "    \n",
    "    dataNorm = scaler.fit_transform(df)\n",
    "    \n",
    "    elbow(df,10)\n",
    "    n_clusters = int(input())\n",
    "    spec = SpectralClustering(n_clusters = n_clusters, assign_labels='discretize', random_state=42).fit(dataNorm)   \n",
    "    \n",
    "    models = pd.DataFrame(id_,columns=['UWI'])\n",
    "    output = pd.concat([df,pd.DataFrame(spec.labels_ +1,columns=['label']),models],axis=1)\n",
    "    \n",
    "    closest = [] \n",
    "    for cl in range(n_clusters):\n",
    "        \n",
    "        df_where_only_i_cluster = output[output.label == cl+1].drop(columns=output.columns[-1])\n",
    "        centroid = df_where_only_i_cluster.mean()\n",
    "        \n",
    "        centroid = np.array(centroid).reshape(1, -1)\n",
    "        centroid, _ = pairwise_distances_argmin_min(centroid,df_where_only_i_cluster)\n",
    "        closest.append(df_where_only_i_cluster.iloc[centroid].index[0])\n",
    "        \n",
    "    print ('Имена центроидных моделей')\n",
    "    for x in closest:        \n",
    "        print(id_[x])\n",
    "    \n",
    "    cl = ['Кластер #'+ str(x+1) for x in range(n_clusters)]\n",
    "    fig1 = px.pie(output,\n",
    "                  names='label',\n",
    "                  color = 'label',\n",
    "                  color_discrete_map = color_palette.loc[:n_clusters].to_dict()[0])\n",
    "                 \n",
    "    fig1.update_layout(title='% из общей ('+ str(len(df)) +') выборки в каждом кластере')\n",
    "    fig1.show()\n",
    "    \n",
    "    divided = []     \n",
    "    for j,jtem in enumerate(np.unique(spec.labels_ +1)):\n",
    "        divided.append(output[output.label == jtem])  \n",
    "    \n",
    "    for i in range(output.shape[1]-1):\n",
    "        feature = output.columns[i]\n",
    "        \n",
    "        fig = make_subplots(rows=1, cols=2)\n",
    "        \n",
    "        for k in range(len(divided)):\n",
    "            #print(divided[k].columns[i])\n",
    "            ecdf = ECDF(divided[k].iloc[:,i].values)\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=ecdf.x,\n",
    "                    y=ecdf.y,\n",
    "                    mode='markers',\n",
    "                    legendgroup=\"group\",\n",
    "                    #name = str(k+1),\n",
    "                    marker = dict(size = 4,color = color_palette.loc[:n_clusters].to_dict()[0][k+1] ),\n",
    "                ),\n",
    "                row = 1, col = 1\n",
    "            )\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x = [divided[k].loc[closest[k]][feature]],\n",
    "                    y = [ecdf.y[list(ecdf.x).index(divided[k].loc[closest[k]][feature])]],\n",
    "                    mode='markers',\n",
    "                    showlegend = False,\n",
    "                    marker_symbol='x',\n",
    "                    marker = dict(size = 15,                                  \n",
    "                                  color = color_palette.loc[:n_clusters].to_dict()[0][k+1] ),\n",
    "                    ),\n",
    "                    row = 1, col = 1\n",
    "                )             \n",
    "              \n",
    "            fig.add_trace(\n",
    "                go.Box(\n",
    "                    name = cl[k],\n",
    "                    y = ecdf.x,\n",
    "                    boxpoints='suspectedoutliers',\n",
    "                    legendgroup=\"group\",   \n",
    "                    showlegend=False,\n",
    "                    marker_color = color_palette.loc[:n_clusters].to_dict()[0][k+1],\n",
    "                ),\n",
    "                row = 1, col =2\n",
    "            )\n",
    "            #print(divided[k].loc[closest[k]][feature],ecdf.y[list(ecdf.x).index(divided[k].loc[closest[k]][feature])])\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x = [cl[k]],\n",
    "                    y = [divided[k].loc[closest[k]][feature]],\n",
    "                    mode='markers',\n",
    "                    showlegend = False,\n",
    "                    marker_symbol='x',\n",
    "                    marker = dict(size = 15,                                  \n",
    "                                  color = color_palette.loc[:n_clusters].to_dict()[0][k+1] ),\n",
    "                    ),\n",
    "                    row = 1, col = 2\n",
    "            )\n",
    "            #fig.update_xaxes(title = 'Значение параметра '+ str(divided[k].columns[i]))\n",
    "            fig.update_layout(\n",
    "                xaxis1=dict(title = \"Значение параметра\"),\n",
    "                xaxis2=dict(title = \"Группа данных\"),\n",
    "                yaxis1=dict(title=\"Доля данных\"),\n",
    "                yaxis2=dict(title=\"Значение параметра\"))\n",
    "            fig.update_layout(title = 'Empirical Cumulative Distribution Function and Box-plots for each cluster ' + str(divided[k].columns[i]))\n",
    "            #fig.write_image('images/'+'Empirical Cumulative Distribution Function of ' + str(divided[k].columns[i])+'.jpeg')\n",
    "\n",
    "        fig.show()\n",
    "        \n",
    "    return output, closest,divided\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inital_output = spec(prepared,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inital_output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates = pd.read_excel('Export2.xlsx',header=None)\n",
    "rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates82 = pd.read_excel('gdhm2021/Sector FLPT, FOPT, FLPR, FOPR.xlsx',header=1)\n",
    "period = rates82.X.tolist()[2:]\n",
    "rates82 = rates82.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prepared_rates82 = pd.DataFrame(columns=['UWI','hlpc','hlpr','hopc','hopr','lpc','lpr','opc','opr'])\n",
    "count = 0\n",
    "\n",
    "for i in range(1,rates82.shape[1],8):\n",
    "    #print(i,i+8)\n",
    "    \n",
    "    df = rates82.iloc[:,i:i+8]    \n",
    "    name = df.loc[0][1].split(sep=',')[0]\n",
    "    #print(name)\n",
    "    #print(df.loc[0][1].split(sep=',')[0])\n",
    "    \n",
    "    prepared_rates82.at[count,'UWI'] =  int(name[7:])    \n",
    "    prepared_rates82.at[count,'hlpc'] = df.iloc[2:,0:1].values.reshape(1,-1)[0].tolist()\n",
    "    prepared_rates82.at[count,'hlpr'] = df.iloc[2:,1:2].values.reshape(1,-1)[0].tolist()\n",
    "    prepared_rates82.at[count,'hopc'] = df.iloc[2:,2:3].values.reshape(1,-1)[0].tolist()\n",
    "    prepared_rates82.at[count,'hopr'] = df.iloc[2:,3:4].values.reshape(1,-1)[0].tolist()\n",
    "    prepared_rates82.at[count,'lpc'] =  df.iloc[2:,4:5].values.reshape(1,-1)[0].tolist()\n",
    "    prepared_rates82.at[count,'lpr'] =  df.iloc[2:,5:6].values.reshape(1,-1)[0].tolist()\n",
    "    prepared_rates82.at[count,'opc'] =  df.iloc[2:,6:7].values.reshape(1,-1)[0].tolist()\n",
    "    prepared_rates82.at[count,'opr'] =  df.iloc[2:,7:8].values.reshape(1,-1)[0].tolist()\n",
    "    count +=1\n",
    "prepared_rates82 = prepared_rates82.sort_values(by='UWI').reset_index(drop = True)\n",
    "prepared_rates82 = prepared_rates82.drop(index = 82)\n",
    "print(prepared_rates82.shape)\n",
    "prepared_rates82.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rates_visual(period,rates,type_prod,output,centroids,color_palette): \n",
    "    \n",
    "    '''\n",
    "    period - даты;\n",
    "    rates - датафрейм параметров добычи;\n",
    "    output - массив с ручками со столбцом номера кластера и идентификатором скважины;\n",
    "    type_prod - наименование параметра добычи, который нужно отобразить\n",
    "                Примеры: 'hlpc','hlpr','hopc','hopr','lpc','lpr','opc','opr';\n",
    "    color_palette- датафрейм цветов для каждого кластера;\n",
    "    '''\n",
    "    #print(color_palette)    \n",
    "    fig = go.Figure()  \n",
    "    fig1 = go.Figure()\n",
    "    start = \"\\033[1m\"\n",
    "    end = \"\\033[0;0m\"\n",
    "    color_center = []\n",
    "    for i in range(rates.shape[0]):\n",
    "        for j in range(len(color_palette)):\n",
    "            if output.label[i] == color_palette.loc[j][0]:\n",
    "                color = color_palette.loc[j][1]\n",
    "                color_center.append(color)\n",
    "                #print(color)\n",
    "                break\n",
    "        \n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                showlegend = False,\n",
    "                x=period,\n",
    "                y=prepared_rates82[type_prod][i],\n",
    "                opacity = 0.5,\n",
    "                #name = str(indexes_data.loc[i][0]),   \n",
    "                name = 'Cluster '+str(color_palette.loc[j][0]),\n",
    "                mode= \"lines\",\n",
    "                line = dict(color = color,width = 0.8),\n",
    "            ))\n",
    "    #print(len(color_center), len(prepared_rates82))\n",
    "    for c in centroids:\n",
    "        #print(c)\n",
    "        fig1.add_trace(\n",
    "                go.Scatter(\n",
    "                    showlegend = True,\n",
    "                    x=period,\n",
    "                    opacity = 0.9,\n",
    "                    name = 'model ' + str(prepared_rates82['UWI'][c]),\n",
    "                    y=prepared_rates82[type_prod][c],\n",
    "                    #name = str(indexes_data.loc[i][0]),\n",
    "                    mode= \"lines\",\n",
    "                    line = dict(color = color_center[c],width = 5),\n",
    "                ))   \n",
    "    for k in range(rates.shape[0]):\n",
    "        #print(i)\n",
    "        fig1.add_trace(\n",
    "            go.Scatter(\n",
    "                showlegend = False,\n",
    "                opacity = 0.2,\n",
    "                x=period,\n",
    "                y=prepared_rates82[type_prod][k],\n",
    "                #name = str(indexes_data.loc[i][0]),   \n",
    "            \n",
    "                mode= \"lines\",\n",
    "                line = dict(color = 'grey',width = 1),\n",
    "            ))      \n",
    "    fig1.update_layout( title= 'Параметр добычи ' + str(type_prod)+' для центроидных моделей кластеров',\n",
    "                         xaxis_title=\"Период добычи\",\n",
    "                         yaxis_title=str(type_prod),\n",
    "                         legend_title=\"Имена моделей\"\n",
    "                        )\n",
    "    fig.update_layout( title= 'Параметр добычи ' + str(type_prod)+' в зависимости от номера кластера',\n",
    "                         xaxis_title=\"Период добычи\",\n",
    "                         yaxis_title=str(type_prod)                         \n",
    "                        )\n",
    "    \n",
    "    return fig.show(),fig1.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates_visual(period,prepared_rates82,'opr',output_initial[0],output_initial[1],color_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=15)\n",
    "scaler = MinMaxScaler()\n",
    "x = scaler.fit_transform(prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = pca.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = x - x.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_.dot(pca.components_.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = (pca.explained_variance_ratio_.astype(list))*100\n",
    "len_var = [i+1 for i in range(len(var))]\n",
    "cs = var.cumsum()\n",
    "\n",
    "variance = pd.DataFrame ()\n",
    "variance['x']= len_var\n",
    "variance['y'] = var\n",
    "variance['y_cum'] = cs\n",
    "variance\n",
    "\n",
    "\n",
    "trace1 = dict(\n",
    "    type='bar',\n",
    "    x=variance['x'],\n",
    "    y=variance['y'],\n",
    "    name='Individual'\n",
    ")\n",
    "\n",
    "trace2 = dict(\n",
    "    type='scatter',\n",
    "    x=variance['x'], \n",
    "    y=variance['y_cum'],\n",
    "    name='Cumulative'\n",
    ")\n",
    "\n",
    "data = [trace1, trace2]\n",
    "\n",
    "layout=dict(\n",
    "    title='Explained variance by different principal components',\n",
    "    yaxis=dict(\n",
    "        title='Explained variance in percent'\n",
    "    ),\n",
    "    xaxis = dict(\n",
    "        title = 'Principal components'\n",
    "    ),\n",
    "    annotations=list([\n",
    "        dict(\n",
    "            x=1.16,\n",
    "            y=1.05,\n",
    "            xref='paper',\n",
    "            yref='paper',\n",
    "            text='Explained Variance',\n",
    "            showarrow=False,\n",
    "        )\n",
    "    ])\n",
    ")\n",
    "\n",
    "\n",
    "fig = go.Figure (data=data, layout = layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_PCA = km(pd.DataFrame(z),target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc(output_PCA[0],output_PCA[1],True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates_visual(period,prepared_rates82,'opr',output_PCA[0],output_PCA[1],color_palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['braycurtis',\n",
    "           'canberra',\n",
    "           'chebyshev',\n",
    "           'cityblock',\n",
    "           'correlation',\n",
    "           'cosine',\n",
    "           'dice',\n",
    "           'euclidean',\n",
    "           'hamming',\n",
    "           'jaccard',\n",
    "           #'jensenshannon',\n",
    "           'kulsinski',\n",
    "           #'mahalanobis',\n",
    "           'matching',\n",
    "           'minkowski',\n",
    "           'rogerstanimoto',\n",
    "           'russellrao',\n",
    "           #'seuclidean',\n",
    "           #'sokalmichene',\n",
    "           'sokalsneath',\n",
    "           'sqeuclidean',\n",
    "           #'yule'\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['euclidean','cosine','hamming']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "dataNorm = scaler.fit_transform(prepared)\n",
    "inital_output = spec(prepared,target)\n",
    "\n",
    "for i,item in enumerate(metrics):    \n",
    "    tsne = TSNE(n_components = 2,\n",
    "                learning_rate = 2,\n",
    "                early_exaggeration=8.0,\n",
    "                metric = item,\n",
    "                perplexity = 5,\n",
    "                n_iter = 1000,            \n",
    "                random_state =42)\n",
    "    \n",
    "    tsne_model=tsne.fit_transform(dataNorm)\n",
    "        \n",
    "    x_axis=tsne_model[:,0]\n",
    "    y_axis=tsne_model[:,1]       \n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x = x_axis,\n",
    "            y = y_axis,        \n",
    "            mode = 'markers', \n",
    "            showlegend=False,\n",
    "            marker = dict(color = inital_output[0].label)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x = x_axis[inital_output[1]],\n",
    "            y = y_axis[inital_output[1]],\n",
    "            mode = 'markers',\n",
    "            showlegend=False,\n",
    "            marker = dict(color = 'green', symbol = 'x', size = 15)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(title = 'TSNE space, ' + item +' metric')\n",
    "    fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,item in enumerate(metrics):   \n",
    "\n",
    "    tsne_model=tsne.fit_transform(dataNorm)\n",
    "    x_axis=tsne_model[:,0]\n",
    "    y_axis=tsne_model[:,1]    \n",
    "    \n",
    "    #output_TSNE = km(pd.DataFrame(tsne_model),target,True)   \n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x = x_axis,\n",
    "            y = y_axis,        \n",
    "            mode = 'markers',           \n",
    "            marker = dict(color = inital_output[0].label)\n",
    "        )\n",
    "    )\n",
    "    fig.update_layout(title = 'TSNE space, metric is ' + item)\n",
    "    fig.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates_visual(period,prepared_rates82,'opr',output_TSNE[0],output_TSNE[1],color_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,item in enumerate(metrics):\n",
    "    \n",
    "    tsne = TSNE(n_components = 2,\n",
    "                learning_rate=200,\n",
    "                early_exaggeration=12.0,\n",
    "                metric = item,\n",
    "                perplexity= 45,\n",
    "                n_iter = 500,            \n",
    "                random_state =42)\n",
    "    \n",
    "    tsne_model=tsne.fit_transform(dataNorm)\n",
    "    x_axis=tsne_model[:,0]\n",
    "    y_axis=tsne_model[:,1]\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x = x_axis,\n",
    "            y = y_axis,\n",
    "            mode = 'markers',\n",
    "            marker= dict(color = output_TSNE[0].label)\n",
    "        )\n",
    "    )\n",
    "    fig.update_layout(title = item)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.umap_ as umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numba --upgrade --ignore-installed --user --no-warn-script-location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install umap_learn --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['euclidean',\n",
    "           'manhattan',\n",
    "           'chebyshev',\n",
    "           'minkowski',\n",
    "           'canberra',\n",
    "           #'braycurtis',\n",
    "           #'mahalanobis',\n",
    "           #'minkowski',\n",
    "           #'seuclidean',\n",
    "           'cosine',\n",
    "           'correlation',\n",
    "           #'haversine',\n",
    "           'hamming',\n",
    "           'jaccard',\n",
    "           'dice',\n",
    "           #'russelrao',\n",
    "           'kulsinski',\n",
    "           #'ll_dirichlet',\n",
    "           'hellinger',\n",
    "           'rogerstanimoto',\n",
    "           'sokalmichener',\n",
    "           'sokalsneath',\n",
    "           'yule'\n",
    "          ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['euclidean','cosine','hamming']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i,item in enumerate(metrics):\n",
    "    scaler = MinMaxScaler()\n",
    "    dataNorm = scaler.fit_transform(prepared)\n",
    "    UM = umap.UMAP(n_neighbors=5,                   \n",
    "                   metric= item,\n",
    "                   random_state=42,\n",
    "                   min_dist=0.1,\n",
    "                   learning_rate=1,                   \n",
    "                   densmap = False,\n",
    "                   n_epochs = 1000).fit_transform(dataNorm)\n",
    "    \n",
    "    inital_output = km(pd.DataFrame(UM),target)\n",
    "    x_axis=UM[:,0]\n",
    "    y_axis=UM[:,1]    \n",
    "\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x = x_axis,\n",
    "            y = y_axis,\n",
    "            mode = 'markers', \n",
    "            marker = dict(color = inital_output[0].label)\n",
    "            \n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x = x_axis[inital_output[1]],\n",
    "            y = y_axis[inital_output[1]],\n",
    "            mode = 'markers',\n",
    "            showlegend=False,\n",
    "            marker = dict(color = 'green', symbol = 'x', size = 15)\n",
    "        )\n",
    "    )\n",
    "    fig.update_layout(title = 'UMAP space, metric is ' + item)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i,item in enumerate(metrics):\n",
    "    scaler = MinMaxScaler()\n",
    "    dataNorm = scaler.fit_transform(prepared)\n",
    "    \n",
    "    x_axis=UM[:,0]\n",
    "    y_axis=UM[:,1]\n",
    "    \n",
    "    output_umap = km(pd.DataFrame(UM),target,True)  \n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x = x_axis,\n",
    "            y = y_axis,\n",
    "            mode = 'markers',\n",
    "            marker = dict(color = output_umap.label)\n",
    "        )\n",
    "    )\n",
    "    fig.update_layout(title = item)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates_visual(period,prepared_rates82,'opr',output_umap[0],output_umap[1],color_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,item in enumerate(metrics):\n",
    "    UM = umap.UMAP(n_neighbors=5,\n",
    "                   metric= item,\n",
    "                   random_state=42,\n",
    "                   min_dist=0.1,\n",
    "                   learning_rate=.90,\n",
    "                   \n",
    "                   densmap = True,\n",
    "                   n_epochs = 200).fit_transform(dataNorm)\n",
    "    x_axis=UM[:,0]\n",
    "    y_axis=UM[:,1]\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x = x_axis,\n",
    "            y = y_axis,\n",
    "            mode = 'markers',\n",
    "            marker = dict(color = output_umap[0].label)\n",
    "        )\n",
    "    )\n",
    "    fig.update_layout(title = item)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['euclidean',\n",
    "           'manhattan',           \n",
    "           'hamming',          \n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pacmap as Pacmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inital_output[0].label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_pacmap = agg(pacmap_model,3,False)\n",
    "\n",
    "for i,item in enumerate(metrics):\n",
    "    print(item)\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    dataNorm = scaler.fit_transform(prepared)\n",
    "    \n",
    "    pacmap = Pacmap.PaCMAP(n_dims= 2,\n",
    "                           n_neighbors = 4, \n",
    "                           distance = item,\n",
    "                           num_iters=3000,\n",
    "                           lr=15)   \n",
    "    \n",
    "    pacmap_model = pacmap.fit_transform(dataNorm)    \n",
    "   \n",
    "    \n",
    "    x_axis=pacmap_model[:,0]\n",
    "    y_axis=pacmap_model[:,1]\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x = x_axis,\n",
    "            y = y_axis,\n",
    "            mode = 'markers',\n",
    "            marker = dict(color = inital_output[0].label)\n",
    "    ))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x = x_axis[inital_output[1]],\n",
    "            y = y_axis[inital_output[1]],\n",
    "            mode = 'markers',\n",
    "            showlegend=False,\n",
    "            marker = dict(color = 'green', symbol = 'x', size = 15)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(title = 'PaCMAP space, metric is ' + item)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i,item in enumerate(metrics):\n",
    "    scaler = MinMaxScaler()\n",
    "    dataNorm = scaler.fit_transform(prepared)\n",
    "    \n",
    "    print(item)    \n",
    "    pacmap = Pacmap.PaCMAP(n_dims= 2,\n",
    "                           n_neighbors = 4, \n",
    "                           distance = item,\n",
    "                           num_iters=3000,\n",
    "                           lr=15)   \n",
    "    \n",
    "    pacmap_model = pacmap.fit_transform(dataNorm)    \n",
    "    \n",
    "    \n",
    "    x_axis=pacmap_model[:,0]\n",
    "    y_axis=pacmap_model[:,1]\n",
    "    \n",
    "    output_pacmap = km(pd.DataFrame(pacmap_model),target,True)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x = x_axis,\n",
    "            y = y_axis,\n",
    "            mode = 'markers',\n",
    "            marker = dict(color = output_pacmap.label)\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(title = item)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost.datasets import titanic\n",
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "train_df = pd.read_excel('synth.xlsx')\n",
    "test_df = pd.read_excel('df1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_value_stats = train_df.isnull().sum(axis=0)\n",
    "null_value_stats[null_value_stats != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.fillna(-999, inplace=True)\n",
    "test_df.fillna(-999, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop('label', axis=1)\n",
    "y = train_df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features_indices = np.where(X.dtypes != np.float)[0]\n",
    "print(X.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=0.5, random_state=42)\n",
    "\n",
    "X_test = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pool = Pool(X_train, y_train, cat_features=categorical_features_indices)\n",
    "validate_pool = Pool(X_validation, y_validation, cat_features=categorical_features_indices)\n",
    "eval_pool = Pool(X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt\n",
    "\n",
    "def hyperopt_objective(params):\n",
    "    model = CatBoostClassifier(\n",
    "        l2_leaf_reg=int(params['l2_leaf_reg']),\n",
    "        learning_rate=params['learning_rate'],\n",
    "        iterations=1000,\n",
    "        eval_metric='Accuracy',\n",
    "        random_seed=42,\n",
    "        verbose=False,\n",
    "        loss_function='Logloss',\n",
    "    )\n",
    "    \n",
    "    cv_data = cv(\n",
    "        Pool(X, y, cat_features=categorical_features_indices),\n",
    "        model.get_params()\n",
    "    )\n",
    "    best_accuracy = np.max(cv_data['test-Accuracy-mean'])\n",
    "    \n",
    "    return 1 - best_accuracy # as hyperopt minimises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import RandomState\n",
    "\n",
    "params_space = {\n",
    "    'l2_leaf_reg': hyperopt.hp.qloguniform('l2_leaf_reg', 0, 2, 1),\n",
    "    'learning_rate': hyperopt.hp.uniform('learning_rate', 1e-2, 3e-1),\n",
    "}\n",
    "\n",
    "trials = hyperopt.Trials()\n",
    "\n",
    "best = hyperopt.fmin(\n",
    "    hyperopt_objective,\n",
    "    space=params_space,\n",
    "    algo=hyperopt.tpe.suggest,\n",
    "    max_evals=30,\n",
    "    trials=trials,\n",
    "    rstate=RandomState(123)\n",
    ")\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Leaf_reaf: ', int(best['l2_leaf_reg']))\n",
    "print ('Learning reate: ', best['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(\n",
    "    l2_leaf_reg=int(best['l2_leaf_reg']),\n",
    "    learning_rate=best['learning_rate'],\n",
    "    \n",
    "    #l2_leaf_reg=3,\n",
    "    #learning_rate=0.0582137369296904,\n",
    "    \n",
    "    iterations=1000,\n",
    "    early_stopping_rounds = 100,\n",
    "    eval_metric='Accuracy',\n",
    "    random_seed=42,\n",
    "    verbose=False,    \n",
    "    loss_function='Logloss', \n",
    "    use_best_model = True,\n",
    ")\n",
    "cv_data = cv(Pool(X, y),  model.get_params(),stratified = True, plot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_pool, eval_set=validate_pool, plot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precise validation accuracy score: {}'.format(np.max(cv_data['test-Accuracy-mean'])))\n",
    "print (model.is_fitted())\n",
    "print (model.tree_count_)\n",
    "print (model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = model.get_feature_importance(train_pool)\n",
    "feature_names = X_train.columns\n",
    "for score, name in sorted(zip(feature_importances, feature_names), reverse=True):\n",
    "    print('{}: {}'.format(name, score))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ABS_SHAP(df_shap,df):\n",
    "    #import matplotlib as plt\n",
    "    # Make a copy of the input data\n",
    "    shap_v = pd.DataFrame(df_shap)\n",
    "    feature_list = df.columns\n",
    "    shap_v.columns = feature_list\n",
    "    df_v = df.copy().reset_index().drop('index',axis=1)\n",
    "    \n",
    "    # Determine the correlation in order to plot with different colors\n",
    "    corr_list = list()\n",
    "    for i in feature_list:\n",
    "        b = np.corrcoef(shap_v[i],df_v[i])[1][0]\n",
    "        corr_list.append(b)\n",
    "    corr_df = pd.concat([pd.Series(feature_list),pd.Series(corr_list)],axis=1).fillna(0)\n",
    "    # Make a data frame. Column 1 is the feature, and Column 2 is the correlation coefficient\n",
    "    corr_df.columns  = ['Variable','Corr']\n",
    "    corr_df['Sign'] = np.where(corr_df['Corr']>0,'red','blue')\n",
    "    \n",
    "    # Plot it\n",
    "    shap_abs = np.abs(shap_v)\n",
    "    k=pd.DataFrame(shap_abs.mean()).reset_index()\n",
    "    k.columns = ['Variable','SHAP_abs']\n",
    "    k2 = k.merge(corr_df,left_on = 'Variable',right_on='Variable',how='inner')\n",
    "    k2 = k2.sort_values(by='SHAP_abs',ascending = True)\n",
    "    colorlist = k2['Sign']\n",
    "    ax = k2.plot.barh(x='Variable',y='SHAP_abs',color = colorlist, figsize=(5,6),legend=False)\n",
    "    ax.set_xlabel(\"SHAP Value (Red = Positive Impact)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "pool = Pool(data=X, label=y, cat_features=None)\n",
    "shap_values = model.get_feature_importance(pool, type='ShapValues')\n",
    "print(shap_values.shape)\n",
    "expected_value = shap_values[0,-1]\n",
    "shap_values = shap_values[:,:-1]\n",
    "print(shap_values.shape)\n",
    "\n",
    "shap.summary_plot(shap_values, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABS_SHAP(shap_values,X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(X.shape[1]):    \n",
    "    shap.dependence_plot(i, shap_values, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_excel('400edit.xlsx').iloc[:,:19]\n",
    "target = pd.read_excel('400edit.xlsx').iloc[:,20:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test_df.iloc[:,1:]\n",
    "target = test_df.iloc[:,1]\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost.utils import get_confusion_matrix\n",
    "get_confusion_matrix(model, eval_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (model.predict(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['UWI'] = np.array(target, dtype ='int32')\n",
    "test['label'] = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['label'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write in a function\n",
    "def shap_plot(j):\n",
    "    explainerModel = shap.TreeExplainer(model)\n",
    "    shap_values_Model = explainerModel.shap_values(test)\n",
    "    p = shap.force_plot(explainerModel.expected_value, shap_values_Model[j], test.iloc[[j]])\n",
    "    return(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_plot(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ABS_SHAP(df_shap,df):\n",
    "    #import matplotlib as plt\n",
    "    # Make a copy of the input data\n",
    "    shap_v = pd.DataFrame(df_shap)\n",
    "    feature_list = df.columns\n",
    "    shap_v.columns = feature_list\n",
    "    df_v = df.copy().reset_index().drop('index',axis=1)\n",
    "    \n",
    "    # Determine the correlation in order to plot with different colors\n",
    "    corr_list = list()\n",
    "    for i in feature_list:\n",
    "        b = np.corrcoef(shap_v[i],df_v[i])[1][0]\n",
    "        corr_list.append(b)\n",
    "    corr_df = pd.concat([pd.Series(feature_list),pd.Series(corr_list)],axis=1).fillna(0)\n",
    "    # Make a data frame. Column 1 is the feature, and Column 2 is the correlation coefficient\n",
    "    corr_df.columns  = ['Variable','Corr']\n",
    "    corr_df['Sign'] = np.where(corr_df['Corr']>0,'red','blue')\n",
    "    \n",
    "    # Plot it\n",
    "    shap_abs = np.abs(shap_v)\n",
    "    k=pd.DataFrame(shap_abs.mean()).reset_index()\n",
    "    k.columns = ['Variable','SHAP_abs']\n",
    "    k2 = k.merge(corr_df,left_on = 'Variable',right_on='Variable',how='inner')\n",
    "    k2 = k2.sort_values(by='SHAP_abs',ascending = True)\n",
    "    colorlist = k2['Sign']\n",
    "    ax = k2.plot.barh(x='Variable',y='SHAP_abs',color = colorlist, figsize=(5,6),legend=False)\n",
    "    ax.set_xlabel(\"SHAP Value (Red = Positive Impact)\")\n",
    "    \n",
    "ABS_SHAP(shap_values,X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = pd.DataFrame()\n",
    "M = test[test.label== 0].reset_index(drop=True)\n",
    "q95,q05 = [],[]\n",
    "\n",
    "for i in range(M.shape[1]):\n",
    "    q95.append(np.quantile(M.iloc[:,i],.95))    \n",
    "    q05.append(np.quantile(M.iloc[:,i],.05))\n",
    "    \n",
    "q05 = pd.DataFrame(q05).T\n",
    "q05.columns = M.columns\n",
    "\n",
    "q95 = pd.DataFrame(q95).T\n",
    "q95.columns = M.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = M.describe().index.tolist()\n",
    "ind.append('q05')\n",
    "ind.append('q95')\n",
    "ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(q05.shape, M.shape,M.index)\n",
    "NOmatched = pd.concat([M.describe(),q05,q95],axis = 0, ignore_index=True)\n",
    "NOmatched.index = ind\n",
    "NOmatched.to_excel('matched.xlsx')\n",
    "NOmatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = first_K.reset_index()\n",
    "c1 = c1.drop(columns = ['index','Case_name','cluster','Unnamed: 0'])\n",
    "c1 = c1.T\n",
    "c1 = c1.reset_index()\n",
    "c1 = c1.drop (columns = 'index')\n",
    "c1 = c1.values.tolist()\n",
    "\n",
    "c2 = second_K.reset_index()\n",
    "c2 = c2.drop(columns = ['index','Case_name','cluster','Unnamed: 0'])\n",
    "c2 = c2.T\n",
    "c2 = c2.reset_index()\n",
    "c2 = c2.drop (columns = 'index')\n",
    "c2 = c2.values.tolist()\n",
    "\n",
    "c3 = third_K.reset_index()\n",
    "c3 = c3.drop(columns = ['index','Case_name','cluster','Unnamed: 0'])\n",
    "c3 = c3.T\n",
    "c3 = c3.reset_index()\n",
    "c3 = c3.drop (columns = 'index')\n",
    "c3 = c3.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = data_copy.columns\n",
    "names = names [1:-1]\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = []\n",
    "df1 = pd.DataFrame()\n",
    "df2 = pd.DataFrame()\n",
    "df2 = pd.DataFrame()\n",
    "\n",
    "for i in range (len(c1)):    \n",
    "    df1 = first_K.reset_index()\n",
    "    df1 = df1.drop(columns = ['index','Case_name','cluster','Unnamed: 0'])    \n",
    "    df1 = df1.reset_index()\n",
    "    df1 = df1.drop (columns = 'index')    \n",
    "df1['rank'] = [1]*len(df1) \n",
    "df1.head()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(c2)):    \n",
    "    df2 = second_K.reset_index()\n",
    "    df2 = df2.drop(columns = ['index','Case_name','cluster','Unnamed: 0'])    \n",
    "    df2 = df2.reset_index()\n",
    "    df2 = df2.drop (columns = 'index')    \n",
    "df2['rank'] = [2]*len(df2)        \n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df1.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listR = [] \n",
    "listS = []\n",
    "lr = []\n",
    "result = []\n",
    "\n",
    "for i in range (len(names)):    \n",
    "    df3 = df3.sort_values(by=names[i])\n",
    "    df3 = df3.reset_index(drop=True)  \n",
    "    df3['ind'] = df3.index+1\n",
    "    df1 = df3[(df3['rank']==1)]\n",
    "    df2 = df3[(df3['rank']==2)]\n",
    "    \n",
    "   \n",
    "    for j in range (df1.shape[0]):\n",
    "        sumR += ((df1.iloc[j,df1.shape[1]-1]-j-1)**2)\n",
    "    listR.append(sumR/len(df2))\n",
    "    sumR = 0\n",
    "    \n",
    "    for j in range (df2.shape[0]):\n",
    "        sumS += ((df2.iloc[j,df2.shape[1]-1]-j-1)**2)\n",
    "    listS.append(sumS/len(df1))\n",
    "    sumS = 0\n",
    "    \n",
    "    w1 = 1/6 + listR[i] + listS[i]\n",
    "    w = 1 / (len(df1)*len(df2)) * w1 - 2/3\n",
    "    result.append(w)\n",
    "    \n",
    "   \n",
    "    r = (len(df1)*len(df2) / (len(df1) + len(df2))) * result[i]\n",
    "    #print (r)\n",
    "    lr.append(r)\n",
    "for i in range(len(lr)):\n",
    "    if lr[i] >=1:\n",
    "        lr[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "for i in range (19):\n",
    "# Import data    \n",
    "    x1 = c1[i]\n",
    "    x2 = c2[i]\n",
    "    #x3 = c3[i]\n",
    "\n",
    "    # Plot\n",
    "    kwargs = dict(hist_kws={'alpha':.3}, kde_kws={'linewidth':2})\n",
    "\n",
    "    plt.figure(figsize=(7,5.5), dpi= 80)\n",
    "    sns.distplot(x1, color=\"dodgerblue\", label=\"cluster 1\", **kwargs)\n",
    "    sns.distplot(x2, color=\"orange\", label=\"cluster 2\", **kwargs)\n",
    "    #sns.distplot(x3, color=\"deeppink\", label=\"cluster 3\", **kwargs)\n",
    "    plt.title (names[i])\n",
    "    \n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len (c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = []\n",
    "r2 = []\n",
    "r3 = []\n",
    "r4 = []\n",
    "#r5 = []\n",
    "#r6 = []\n",
    "for i in range (len(c1)):      \n",
    "    d = stats.kruskal(c1[i], c2[i])#, c3[i])\n",
    "    ks = stats.ks_2samp(c1[i],c2[i])\n",
    "    mann = stats.mannwhitneyu(c1[i],c2[i])   \n",
    "    r1.append(d)\n",
    "    r2.append(ks)\n",
    "    r3.append(mann)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "re1 =[]\n",
    "re2 =[]\n",
    "re3 =[]\n",
    "re4 =[]\n",
    "#re5 = []\n",
    "for i in range(19):   \n",
    "    re1.append(r1[i][1])\n",
    "    re2.append(r2[i][1])\n",
    "    re3.append(r3[i][1])\n",
    "    #re4.append (r4[i][1])\n",
    "    #re5.append (r5[i][1])\n",
    "res = {'Kruskal H-Test ': re1,\n",
    "       'KS Test':re2,\n",
    "       'MannWhitney U-Test':re3,\n",
    "       'Lehmann-Rosenblatt':lr,\n",
    "       'Name of Parameter': names}\n",
    "resu = pd.DataFrame (res)\n",
    "resu.to_excel('Distribution_KMeans.xlsx')\n",
    "for i in range (len(resu)):\n",
    "    resu.iloc[:,4][i] = resu.iloc[:,4][i].replace('$','')\n",
    "\n",
    "resu = resu.round(5)\n",
    "resu.head()\n",
    "print (resu.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range (resu.shape[1]-1):\n",
    "  \n",
    "    if i == 3:\n",
    "        threshold = 0.46\n",
    "    else:\n",
    "        threshold = 0.05  \n",
    "        \n",
    "    plt.figure(figsize=(15,5), dpi= 80)\n",
    "    ax = plt.gca()\n",
    "    #ax.invert_yaxis()\n",
    "    plt.bar(resu.iloc[:,4], resu.iloc[:,i], alpha = 0.7)\n",
    "    plt.axes()\n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.title (resu.columns[i], fontsize = 20)\n",
    "    ax.plot([-1, len(resu)], [threshold, threshold], \"k--\",c = 'red')\n",
    "    plt.ylabel('pValue', size =20)\n",
    "    plt.xticks(size =15)\n",
    "    plt.yticks(size =15)\n",
    "    resu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resu.iloc[:,1] < threshold\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kmeans1(title, clust, j):\n",
    "    s=10\n",
    "    #figk1 = go.Figure()\n",
    "    figk1.add_trace(\n",
    "        go.Scatter(x=list(dataK[\"Case_name\"]),\n",
    "                   y=clust,\n",
    "                   name=\"hundle\",\n",
    "                   mode= \"markers\",\n",
    "                   text=dataK['cluster'],                  \n",
    "                   marker=dict(size=s,color = dataK['cluster'])),\n",
    "        row=j,\n",
    "        col=1\n",
    "       )\n",
    "    figk1.update_yaxes(title_text=title, row=j, col=1)\n",
    "    figk1.update_layout(showlegend=False)\n",
    "     \n",
    "    return (figk1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_handles = ['$Fault_20', '$OWC', '$Corey_O_W', '$Corey_water', '$Krw_Sorw', '$gamma_poro', '$beta_poro',\n",
    "'$LN_gamma_perm', '$LN_beta_perm', '$LN_gamma_swcr', '$LN_beta_swl', '$cos_teta', '$LN_gamma_sw',\n",
    "'$LN_beta_sw', '$ANI', '$Azimuth', '$Major', '$Minior', '$Vertical']\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "figk1 = make_subplots(rows=19, cols=1)\n",
    "figk1.update_layout(height=8500)\n",
    "\n",
    "j = 1\n",
    "for i in list_of_handles:\n",
    "    clust = dataK[i]\n",
    "    title = i\n",
    "    figk1 = Kmeans1(title, clust, j)\n",
    "    j += 1\n",
    "\n",
    "py.offline.plot(figk1, filename='Kmeans(hundles).html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(title, clust_1, clust_2, clust_3): \n",
    "    \n",
    "    trace0=go.Box(\n",
    "        #x=x,\n",
    "        y=clust_1,\n",
    "        name = \"cluster1\",\n",
    "        width=0.5,\n",
    "        marker=dict(\n",
    "            color = 'green',\n",
    "        )\n",
    "    )\n",
    "\n",
    "    trace1=go.Box(\n",
    "        #x=x,\n",
    "        y=clust_2,\n",
    "        name = \"cluster2\",\n",
    "        width=0.5,\n",
    "        marker=dict(\n",
    "            color = 'black',\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    trace2=go.Box(\n",
    "        #x=x,\n",
    "        y=clust_3,\n",
    "        name = \"cluster3\",\n",
    "        width=0.5,\n",
    "\n",
    "        marker=dict(\n",
    "            color = 'red',\n",
    "        )\n",
    "               \n",
    "    )\n",
    "    \n",
    "    data = [trace0, trace1, trace2]\n",
    "\n",
    "    layout = go.Layout(\n",
    "        yaxis=dict(\n",
    "            title=title,\n",
    "            zeroline=False\n",
    "        ),\n",
    "        boxmode='group'\n",
    "    )\n",
    "\n",
    "\n",
    "    fig_K = go.Figure(data=data, layout=layout) \n",
    "    return (fig_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_of_handles = ['$Fault_20', '$OWC', '$Corey_O_W', '$Corey_water', '$Krw_Sorw', '$gamma_poro', '$beta_poro',\n",
    "'$LN_gamma_perm', '$LN_beta_perm', '$LN_gamma_swcr', '$LN_beta_swl', '$cos_teta', '$LN_gamma_sw',\n",
    "'$LN_beta_sw', '$ANI', '$Azimuth', '$Major', '$Minior', '$Vertical']\n",
    "\n",
    "fig_K = make_subplots(rows=19, cols=3)\n",
    "\n",
    "for i in list_of_handles:\n",
    "    clust_1 = first_K[i]\n",
    "    clust_2 = second_K[i]\n",
    "    clust_3 = third_K[i]\n",
    "    title = i    \n",
    "    fig_K = kmeans(title,clust_1, clust_2, clust_3)\n",
    "    fig_K.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Перед загрузкой датафрейма добавить пустую строку сверху документа вручную\n",
    "td = pd.read_excel('rates.xlsx', header=None)\n",
    "td.drop(td.columns[[0,4]], axis =1, inplace =True)\n",
    "td.columns = ['Name','Days','OPR'] \n",
    "#dataset 193 ячейки для одного графика\n",
    "#Петрель выгружает модели в соответсвии порядковым номером "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#добавляем датасет с историей добычи\n",
    "hs=pd.read_excel('history.xlsx')\n",
    "hs.drop(hs.columns[[0,1]], axis =1, inplace =True)\n",
    "hs.columns = ['Days','OPR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Для поисках моделей у центроида необходимо знать:\n",
    "#  1) case name в геологическом датасете (см.выше для каждого метода, там, где мы определили индексы этих моделей)\n",
    "#                  print (название датасета.стобец имён моделей[индекс модели])\n",
    "    # EXAMPLE:\n",
    "                            # In:  print (dataUM.Case_name[45:46])\n",
    "                            # Out: 54    Sector_17_09_844\n",
    "                            #            Name: Case_name, dtype: object\n",
    "\n",
    "#  2) имя модели в продакшн датасете\n",
    "td.query(\"Name == 'SECTOR_17_09_871 : (Re-sampled)'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Days = td.Days.values[11:204] #Constant values for each model\n",
    "col1 = {'col1': Days}\n",
    "Days = pd.DataFrame (data = col1)\n",
    "Days = Days.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Функция ищет названия центроидных моделей после Кmeans кластеризации и соответсвующие им X,Y координаты в датасете с дебетами\n",
    "# и записывает их в новые датафреймы\n",
    "def f(num):\n",
    "    temp_arr = td.Name.values.astype(str) \n",
    "    \n",
    "    return np.where(np.char.find(temp_arr, '_'+str(num)) != -1)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "temp = dataK.Case_name.values[closest].astype(str)\n",
    "x = []\n",
    "y = []\n",
    "for string in temp:\n",
    "    num = string.split('_')[-1]\n",
    "    index = f(num)[0][0]    \n",
    "    x.append(td.Days.values[index+11:index+204])\n",
    "    y.append(td.OPR.values[index+11:index+204])\n",
    "#y[0]\n",
    "x_c1 = x[0]\n",
    "x_c2 = x[1]\n",
    "#x_c3 = x[2]\n",
    "y_c1 = y[0]\n",
    "y_c2 = y[1]\n",
    "#y_c3 = y[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (stats.ks_2samp (y_c1, y_c2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.f_oneway.html\n",
    "print (stats.f_oneway (y_c1, y_c2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cent = {'KM_c1': y_c1, 'KM_c2':y_c2}\n",
    "cents = pd.DataFrame(cent)\n",
    "cents.to_excel ('cKmeans.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rates(i,j,f):\n",
    "    s=10\n",
    "    if f ==204: # число должно соответствовать количеству моделей в датасете(лишь для красоты)\n",
    "        fig1.add_trace(\n",
    "            go.Scatter(\n",
    "                showlegend=True,\n",
    "                x=x_c1,\n",
    "                y=y_c1,\n",
    "                name = \"1st cluster\",\n",
    "                marker = dict(color = 'blue',\n",
    "                    size = 12),\n",
    "                mode = 'lines'))\n",
    "        \n",
    "        fig1.add_trace(\n",
    "            go.Scatter(            \n",
    "                x=x_c2,\n",
    "                y=y_c2,\n",
    "                name = \"2nd cluster\",\n",
    "                marker = dict(color = 'yellow',\n",
    "                     size = 12),\n",
    "                mode = 'lines'))\n",
    "    \n",
    "   #     fig1.add_trace(\n",
    "   #         go.Scatter(            \n",
    "    #           x=x_c3,\n",
    "    #           y=y_c3,\n",
    "    #            name = \"3rd cluster\",\n",
    "    #            marker = dict(color = 'green',\n",
    "    #            size = 12),\n",
    "    #            mode = 'lines')) \n",
    "    \n",
    "        fig1.add_trace(\n",
    "            go.Scatter(                       \n",
    "                x=hs.Days[10:202],\n",
    "                y=hs.OPR[10:202],\n",
    "                name = \"History\",\n",
    "                marker = dict(color = 'red',\n",
    "                              size = 20),\n",
    "                \n",
    "                mode = 'lines'))\n",
    "        \n",
    "    fig1.add_trace(\n",
    "        go.Scatter(\n",
    "            showlegend=False,\n",
    "            x=td.Days[i:j],\n",
    "            y=td.OPR[i:j],\n",
    "            opacity = 0.3,\n",
    "                   #name=\"hundle\",\n",
    "            mode= \"lines\",\n",
    "            marker = dict(\n",
    "                color = 'grey')\n",
    "            #text=td.Name[],\n",
    "        )  \n",
    "       )\n",
    "    fig1.update_layout(showlegend=False)\n",
    "    return (fig1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 11\n",
    "j = 205\n",
    "\n",
    "fig1 = go.Figure()\n",
    "for k in range (len(td)):\n",
    "    if i>len(td) and j > len(td):\n",
    "        break        \n",
    "    fig1 = rates(i,j,k)\n",
    "    #print (k)    \n",
    "    i = i+205\n",
    "    j = j+205\n",
    "    \n",
    "fig1.update_layout(showlegend=True)\n",
    "\n",
    "py.offline.plot(fig1, filename='Rates(Kmeans clustering).html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Подгужаю названия моделей из датасета с дебетами\n",
    "n_list = td.Name.unique()\n",
    "name_list =pd.DataFrame (data = n_list)\n",
    "name_list.drop([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24], inplace =True)\n",
    "name_list = name_list.reset_index(drop = True)\n",
    "name_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Данная функция использует модели, хранящиеся в датасете с номерами кластеров (по результатам геологической кластеризации)\n",
    "#   Затем использья ключ, мы находим соответствующие дебеты для каждой модели\n",
    "#   И записываем в новый датасет дебеты, где названием столбцов служат имена соответсвущих моделей\n",
    "#   X-координата (даты) едины для всех кейсов и хратятся в датасете Days \n",
    "\n",
    "def f_sum(K):\n",
    "    summary = pd.DataFrame()\n",
    "\n",
    "    td_names = td.Name.values.astype(str)\n",
    "    k_names = K.Case_name.values\n",
    "    for i in range(K.shape[0]):\n",
    "        model_name = str(k_names[i][-3:])# этой строкой мы выбираем ключевое слово, согласно которому идет поиск        \n",
    "        index = int(np.where(np.char.find(td_names, '_'+ model_name) != -1)[0])    \n",
    "        summary[\"Rate_\" + str(k_names[i])] = td.OPR.values[index+11:index+204]\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "print(utils.f_sum_1)\n",
    "\n",
    "from utils import f_sum_1\n",
    "print(f_sum_1)\n",
    "\n",
    "from utils import *\n",
    "print(f_sum_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(utils.f_sum_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Указать рабочий датасет,содержащий модели нужного кластера после выбранного метода преобразования\n",
    "rates_first_Kmeans = pd.DataFrame (f_sum(first_K))\n",
    "rates_first_Kmeans\n",
    "print (rates_first_Kmeans.shape)\n",
    "\n",
    "rates_second_Kmeans = pd.DataFrame (f_sum(second_K))\n",
    "rates_second_Kmeans\n",
    "print (rates_second_Kmeans.shape)\n",
    "\n",
    "rates_third_Kmeans = pd.DataFrame (f_sum(third_K))\n",
    "rates_third_Kmeans\n",
    "print (rates_third_Kmeans.shape)\n",
    "\n",
    "if rates_first_Kmeans.shape[1]== first_K.shape[0]:\n",
    "    print ('Correct')\n",
    "else:\n",
    "    print ('Error')\n",
    "\n",
    "if rates_second_Kmeans.shape[1]== second_K.shape[0]:\n",
    "    print ('Correct')\n",
    "else:\n",
    "    print ('Error')\n",
    "\n",
    "if rates_third_Kmeans.shape[1]== third_K.shape[0]:\n",
    "    print ('Correct')\n",
    "else:\n",
    "    print ('Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=Days.col1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(num):\n",
    "    temp_arr = td.Name.values.astype(str) \n",
    "    \n",
    "    return np.where(np.char.find(temp_arr, '_'+str(num)) != -1)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "temp = dataK.Case_name.values[closest].astype(str)\n",
    "x = []\n",
    "y = []\n",
    "for string in temp:\n",
    "    num = string.split('_')[-1]\n",
    "    index = f(num)[0][0]    \n",
    "    x.append(td.Days.values[index+11:index+204])\n",
    "    y.append(td.OPR.values[index+11:index+204])\n",
    "#y[0]\n",
    "x_c1 = x[0]\n",
    "x_c2 = x[1]\n",
    "#x_c3 = x[2]\n",
    "y_c1 = y[0]\n",
    "y_c2 = y[1]\n",
    "#y_c3 = y[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rates(k, flag, leg):\n",
    "    \n",
    "    x=Days.col1# у каждом случае длина ряда Х меняется и надо вручную проверять\n",
    "    s = 40\n",
    "    \n",
    "    if flag == 1:\n",
    "        fig1.add_trace(\n",
    "            go.Scatter(\n",
    "                showlegend=leg,\n",
    "                x=x,\n",
    "                y=k,\n",
    "                name = \"1st cluster\",\n",
    "                opacity = 0.2,\n",
    "                mode= \"lines\",\n",
    "                marker = dict(\n",
    "                    color = 'blue')                \n",
    "            )  \n",
    "        )       \n",
    "    if flag == 2:\n",
    "        fig1.add_trace(\n",
    "            go.Scatter(\n",
    "                showlegend=leg,\n",
    "                x=x,\n",
    "                y=k,\n",
    "                name = \"2st cluster\",\n",
    "                opacity = 0.2,\n",
    "                mode= \"lines\",\n",
    "                marker = dict(\n",
    "                    color = 'orange')\n",
    "            )  \n",
    "        ) \n",
    "       \n",
    " #   if flag == 3:\n",
    " #       fig1.add_trace(\n",
    " #           go.Scatter(\n",
    " #               showlegend=leg,\n",
    " #               x=x,\n",
    " #               y=k,\n",
    " #               name = \"3st cluster\",\n",
    " #               opacity = 0.2,\n",
    " #               mode= \"lines\",\n",
    "  #              marker = dict(\n",
    " #                   color = 'green')  \n",
    " #           ) \n",
    "#        )\n",
    "    if flag == 4:\n",
    "        fig1.add_trace(\n",
    "            go.Scatter(\n",
    "                showlegend=True,\n",
    "                x=x_c1,\n",
    "                y=y_c1,\n",
    "                name = \"1st cluster\",\n",
    "                marker = dict(color = 'blue',\n",
    "                              size = s),\n",
    "                mode = 'lines',\n",
    "                line = dict (width =4)\n",
    "            \n",
    "            )\n",
    "        )\n",
    "        \n",
    "        fig1.add_trace(\n",
    "            go.Scatter(\n",
    "                showlegend=True,\n",
    "                x=x_c2,\n",
    "                y=y_c2,\n",
    "                name = \"2nd cluster\",\n",
    "                marker = dict(color = 'yellow',\n",
    "                              size = s),\n",
    "                mode = 'lines',\n",
    "                line = dict (width =4)\n",
    "            )\n",
    "        )\n",
    "    \n",
    "#        fig1.add_trace(\n",
    " #           go.Scatter(\n",
    " #               showlegend=True,\n",
    "#                x=x_c3,\n",
    " #               y=y_c3,\n",
    "#                name = \"3rd cluster\",\n",
    "#                marker = dict(color = 'green',\n",
    "#                              size = s),\n",
    "#                mode = 'lines',\n",
    "#                line = dict (width =4)\n",
    "#            )\n",
    "#       ) \n",
    "    \n",
    "        fig1.add_trace(\n",
    "            go.Scatter(\n",
    "                showlegend=True,\n",
    "                x=hs.Days[10:202],\n",
    "                y=hs.OPR[10:202],\n",
    "                name = \"History\",\n",
    "                marker = dict(color = 'red',\n",
    "                              size = s),\n",
    "                \n",
    "                mode = 'lines',\n",
    "                line = dict (width =4)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "       \n",
    "    #fig1.update_layout(showlegend=False)\n",
    "    return (fig1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = go.Figure()\n",
    "       \n",
    "fl = False\n",
    "l = True    \n",
    "      \n",
    "\n",
    "for k0 in range (len(rates_first_Kmeans.columns)): \n",
    "    flag = 1\n",
    "    if k0 == len(rates_first_Kmeans.columns):\n",
    "        leg = l\n",
    "    else:\n",
    "        leg = fl\n",
    "    fig1 = rates(rates_first_Kmeans.iloc[:,k0], flag, leg)\n",
    "        \n",
    "        \n",
    "for k1 in range (len(rates_second_Kmeans.columns)): \n",
    "    flag = 2\n",
    "    if k1 == len(rates_second_Kmeans.columns):\n",
    "        leg = fl\n",
    "    else:\n",
    "        leg = fl\n",
    "    fig1 = rates(rates_second_Kmeans.iloc[:,k1], flag, leg)\n",
    "         \n",
    "        \n",
    "    \n",
    "#for k2 in range (len(rates_third_Kmeans.columns)): \n",
    "#    flag = 3\n",
    "#    if k2 == len(rates_third_Kmeans.columns): \n",
    "#        leg = fl  \n",
    "#    else:\n",
    "#        leg = fl\n",
    "        \n",
    "#    fig1 = rates(rates_third_Kmeans.iloc[:,k2], flag, leg) \n",
    "\n",
    "fig1 = rates(rates_first_Kmeans.iloc[:,0], flag = 4, leg = True)      \n",
    "\n",
    "py.offline.plot(fig1, filename='Rates(Kmeans clustering).html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kolmagorov-Smirnov test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, lr=0.01, epochs=10):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Our fit function trains on the dataset X and tries to predict vector y,\n",
    "        Using the learning rate, it will modify it's weight vector to increase\n",
    "        it's accuracy in predictions.\n",
    "        It will iterate over the X dataset as defined by the epochs.\n",
    "        Args:\n",
    "            X: The input data (numpy array of shape [n_samples * m_features])\n",
    "            y: Class labels vector (numpy array of shape [n_samples])\n",
    "        \"\"\"\n",
    "        # a vector of floats between 0 and 1\n",
    "        weights = np.random.rand(X.shape[1],)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            # list of predicted classes for our accuracy calculation\n",
    "            predicted = []\n",
    "            for i_index, sample in enumerate(X):\n",
    "                y_hat = self.predict(sample, weights)\n",
    "                predicted.append(y_hat)  # add our new prediction to the array\n",
    "                for j_index, feature in enumerate(weights):\n",
    "                    # update our weight values\n",
    "                    delta = self.lr * (y[i_index] - y_hat)\n",
    "                    delta = delta * sample[j_index-1]\n",
    "                    weights[j_index-1] = weights[j_index-1] + delta\n",
    "            print('[Epoch {ep}] Accuracy: {acc}'.format(\n",
    "                ep=epoch, acc=self._calculate_accuracy(y, predicted)\n",
    "            ))\n",
    "\n",
    "    def _calculate_accuracy(self, actual, predicted):\n",
    "        \"\"\"\n",
    "        Calculate the accuracy of predictions for this epoch.\n",
    "        Args:\n",
    "            actual: vector of actual class values (the y vector) [n_samples]\n",
    "            predicted: vector of predicted class values [n_samples]\n",
    "        \"\"\"\n",
    "        return sum(np.array(predicted) == np.array(actual)) / float(len(actual))\n",
    "\n",
    "    def predict(self, x, w):\n",
    "        \"\"\"\n",
    "        Create a binary prediction from an activation function on the data\n",
    "        sample and the weight vector.\n",
    "        Args:\n",
    "            x: vector of the data sample - shape [m_features]\n",
    "            w: vector of the weights - shape [m_features]\n",
    "        Returns:\n",
    "            0 or 1\n",
    "        \"\"\"\n",
    "        res = self._sum(x, w)\n",
    "        return 1 if res > 0.0 else 0.0\n",
    "\n",
    "    def _sum(self, x, w):\n",
    "        \"\"\"\n",
    "        Multiply our sample and weight vector elements then the sum of the\n",
    "        result.\n",
    "        Args:\n",
    "            x: vector of the data sample - shape [m_features]\n",
    "            w: vector of the weights - shape [m_features]\n",
    "        Returns:\n",
    "            Int of the sum of vector products\n",
    "        \"\"\"\n",
    "        return np.sum(np.dot(x, np.transpose(w)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d = pd.read_pickle('input/Дельтовый канал.pkl')\n",
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(d[0],np.arange(0,len(d[0]),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
